{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Location Grouping Multivariate LSTM\n",
        "\n"
      ],
      "metadata": {
        "id": "GSpWZfknkISQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Nida Copty, India Tory, Emily Nguyen, Thomas Nguyen"
      ],
      "metadata": {
        "id": "-Tr9xTLxkWo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook represents the next evolution in our LSTM modeling journey, building upon the foundation laid by the univariate LSTM model and the multivariate LSTM model utilizing city-wide occupancy rate data. Now, our focus shifts to training the model on individual shelter data, aiming to capture unique patterns inherent to each shelter.\n",
        "\n",
        "To avoid overfitting, we employ a strategic approach to encode shelter data using one-hot vectors. With over 200 shelters, a direct one-hot encoding would lead to excessive feature dimensions, risking over-parameterization. Thus, we devise a grouping strategy, leveraging geolocation information to cluster shelters with similar characteristics into cohesive groups.\n",
        "\n",
        "The key challenge lies in determining the optimal number of clusters for grouping shelters. We adopt a data-driven approach, employing k-means clustering with varying centroids. Through varying methods of analysis, we aim to discern the ideal number of centroids that best capture the nuanced patterns within the shelter data."
      ],
      "metadata": {
        "id": "f67z6tHFkbY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "zg4WGHSBnm9G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XsZttXVJue9s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy as dc\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from geopy.geocoders import Nominatim\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Load"
      ],
      "metadata": {
        "id": "fyl9JnTFnqrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Occupancy Rate (Output Data):\n",
        "data_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv\"\n",
        "data_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv\"\n",
        "data_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv\"\n",
        "data_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv\"\n",
        "links = [data_24, data_23, data_22, data_21]\n",
        "\n",
        "#Weather Data\n",
        "data_w_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv\"\n",
        "data_w_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv\"\n",
        "data_w_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv\"\n",
        "data_w_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv\"\n",
        "links_weather = [data_w_24, data_w_23, data_w_22, data_w_21]\n",
        "\n",
        "#Housing\n",
        "data_housing = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv\"\n",
        "\n",
        "#Crisis helpline\n",
        "data_crisis = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv\""
      ],
      "metadata": {
        "id": "DO_1qLFjnsD4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to read and report on errors when reading csv files."
      ],
      "metadata": {
        "id": "LGPKL1Rznyi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_to_pandas(file_path):\n",
        "    try:\n",
        "        # Load CSV file into a pandas dataFrame\n",
        "        df = pd.read_csv(file_path, header=0, low_memory=False, encoding='unicode_escape')\n",
        "        print(\"Number of rows in the dataFrame:\", file_path, len(df))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None"
      ],
      "metadata": {
        "id": "HIOQXmDQn1FZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function below is to convert all the datasets of different features into a singular panda dataframes and also a hashmap containing individual shelter datas."
      ],
      "metadata": {
        "id": "ZhZiETYkn4kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(output_data, weather_data, housing, crisis):\n",
        "\n",
        "    #-------Output Data-------#\n",
        "    #Loading up the links to the output dataset\n",
        "    for i in range(len(output_data)):\n",
        "        output_data[i] = load_csv_to_pandas(output_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for output datasets\n",
        "    for i in range(len(output_data)):\n",
        "        #print(output_data[i])\n",
        "        output_data[i] = output_data[i].drop(columns = ['_id', 'ORGANIZATION_ID', 'SHELTER_ID', 'LOCATION_ID', 'LOCATION_CITY', 'LOCATION_PROVINCE', 'PROGRAM_NAME', 'SECTOR', 'PROGRAM_MODEL','OVERNIGHT_SERVICE_TYPE', 'PROGRAM_AREA', 'SERVICE_USER_COUNT', 'CAPACITY_FUNDING_BED', 'UNOCCUPIED_BEDS', 'UNAVAILABLE_BEDS', 'CAPACITY_FUNDING_ROOM', 'UNOCCUPIED_ROOMS', 'UNAVAILABLE_ROOMS'])\n",
        "        output_data[i]['OCCUPANCY_DATE'] = output_data[i]['OCCUPANCY_DATE']\n",
        "        output_data[i]['OCCUPANCY_DATE'] =  pd.to_datetime(output_data[i]['OCCUPANCY_DATE'], format='%Y-%m-%d')\n",
        "\n",
        "    #Joining the Output data together\n",
        "    big_data = output_data[0]\n",
        "    for i in range(1,len(output_data)):\n",
        "        big_data = pd.concat([big_data, output_data[i]], ignore_index = True)\n",
        "\n",
        "    #Determine the max and min date in the dataset to create a date vector to fill out empty values\n",
        "    max_date = big_data['OCCUPANCY_DATE'].max()\n",
        "    min_date = big_data['OCCUPANCY_DATE'].min()\n",
        "    date_range = pd.date_range(start=min_date, end=max_date, freq = 'D')\n",
        "    date_df = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
        "\n",
        "    #-------Weather Data-------#\n",
        "\n",
        "    #loading up the links to the weather dataset\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = load_csv_to_pandas(weather_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for weather datasets\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = weather_data[i].drop(columns = ['ï»¿\"Longitude (x)\"', 'Latitude (y)', 'Station Name', 'Climate ID', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp Flag', 'Min Temp Flag', 'Mean Temp Flag', 'Heat Deg Days Flag', 'Cool Deg Days Flag', 'Total Rain (mm)', 'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag', 'Total Precip Flag',\n",
        "        'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)', 'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)', 'Spd of Max Gust Flag'])\n",
        "        weather_data[i]['Date/Time'] = weather_data[i]['Date/Time'].astype(str)\n",
        "        weather_data[i]['Date/Time'] = pd.to_datetime(weather_data[i]['Date/Time'])\n",
        "\n",
        "    #Joining the Weather data together\n",
        "    big_weather = weather_data[0]\n",
        "    for i in range(1, len(weather_data)):\n",
        "        big_weather = pd.concat([big_weather, weather_data[i]], ignore_index = True)\n",
        "\n",
        "    #Cut down all data with dates that is bigger than the biggest date and smaller than the smallest date with an output\n",
        "    big_weather = big_weather[big_weather['Date/Time'] <= max_date]\n",
        "    big_weather = big_weather[big_weather['Date/Time'] >= min_date]\n",
        "\n",
        "    #Fill out datasets' entries w no data w 0\n",
        "    big_weather = big_weather.fillna(0)\n",
        "\n",
        "    #Changing non output dataset's date column to 'OCCUPANCY_DATE'\n",
        "    big_weather = big_weather.rename(columns = {'Date/Time': 'OCCUPANCY_DATE'})\n",
        "\n",
        "    #-------Housing Data-------#\n",
        "\n",
        "    #loading up housing data\n",
        "    housing = load_csv_to_pandas(housing)\n",
        "\n",
        "    #Dropping irrelevant columns for housing dataset\n",
        "    housing = housing[housing['GEO'] == 'Toronto, Ontario']\n",
        "    housing = housing[housing['New housing price indexes'] == 'Total (house and land)']\n",
        "    housing = housing.drop(columns = ['GEO', 'DGUID', 'New housing price indexes', 'UOM', 'UOM_ID', 'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'STATUS', 'SYMBOL', 'TERMINATED', 'DECIMALS'])\n",
        "    housing = housing.rename(columns = {housing.columns[0]: 'OCCUPANCY_DATE'})\n",
        "    housing[\"OCCUPANCY_DATE\"] = pd.to_datetime(housing[\"OCCUPANCY_DATE\"])\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] <= max_date].reset_index(drop=True)\n",
        "    housing = pd.merge(housing, date_df, on = 'OCCUPANCY_DATE', how = 'outer')\n",
        "    housing = housing.sort_values(by='OCCUPANCY_DATE').reset_index(drop=True)\n",
        "    housing = housing.ffill()\n",
        "\n",
        "    #-------Crisis Data-------#\n",
        "\n",
        "    #Loading the crisis dataset\n",
        "    crisis = load_csv_to_pandas(crisis)\n",
        "\n",
        "    #Analyize Data\n",
        "    crisis = crisis.drop(columns = ['ï»¿OBJECTID', 'EVENT_ID', 'EVENT_YEAR', 'EVENT_MONTH', 'EVENT_DOW', 'EVENT_HOUR', 'DIVISION', 'OCCURRENCE_CREATED', 'APPREHENSION_MADE', 'MCIT_ATTEND', 'HOOD_158', 'NEIGHBOURHOOD_158', 'HOOD_140', 'NEIGHBOURHOOD_140'])\n",
        "    crisis = crisis.rename(columns = {'EVENT_DATE': 'OCCUPANCY_DATE'})\n",
        "    crisis = crisis.groupby(['OCCUPANCY_DATE', 'EVENT_TYPE']).size().unstack(fill_value=0)\n",
        "    crisis.reset_index(inplace=True)\n",
        "    crisis = crisis.rename_axis(None, axis=1)\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE']).dt.date\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE'])\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] <= max_date]\n",
        "    crisis = pd.merge(date_df, crisis, on='OCCUPANCY_DATE', how='left')\n",
        "\n",
        "    #-------Final Data Prep-------#\n",
        "\n",
        "    #Merge the datasets together through date\n",
        "    big_data = pd.merge(big_data, big_weather, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, housing, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, crisis, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "\n",
        "    big_data = big_data.sort_values(by='OCCUPANCY_DATE')\n",
        "\n",
        "    #Placing the bed and room occupancy column last\n",
        "    room_occupancy = big_data.pop('OCCUPANCY_RATE_ROOMS')\n",
        "    bed_occupancy = big_data.pop('OCCUPANCY_RATE_BEDS')\n",
        "    big_data['OCCUPANCY_RATE_BEDS'] = bed_occupancy\n",
        "    big_data['OCCUPANCY_RATE_ROOMS'] = room_occupancy\n",
        "\n",
        "    grouped_data = big_data.groupby('PROGRAM_ID')\n",
        "    shelter_data_frames = {}\n",
        "    for shelter_id, shelter_group in grouped_data:\n",
        "        shelter_data_frames[shelter_id] = shelter_group\n",
        "        shelter_data_frames[shelter_id]['OCCUPANCY_DATE'] = pd.to_datetime(shelter_data_frames[shelter_id]['OCCUPANCY_DATE'])\n",
        "\n",
        "    big_data.reset_index(inplace=True)\n",
        "    big_data = big_data.drop(columns = ['index'])\n",
        "\n",
        "    return big_data, shelter_data_frames"
      ],
      "metadata": {
        "id": "c6_-Uckvn7sh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the function to get the dataframe and hashmap"
      ],
      "metadata": {
        "id": "9oPW0KT6oGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe, iso_data = loadData(links.copy(), links_weather.copy(), data_housing, data_crisis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bFh1sQ8oJLm",
        "outputId": "c5208d94-d101-4cf3-ffdc-5e2e7a0dffac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv 11459\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv 48345\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv 49478\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv 50944\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv 366\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv 62160\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv 291991\n"
          ]
        }
      ]
    }
  ]
}