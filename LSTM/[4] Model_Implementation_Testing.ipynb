{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGQy35rUlcRa"
      },
      "source": [
        "# Model Implementation Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejxryfnymb4H"
      },
      "source": [
        "By Nida Copty, India Tory, Emily Nguyen, Thomas Nguyen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nja30yAXm15f"
      },
      "source": [
        "This notebook marks the culmination of our efforts in constructing LSTM models to forecast homeless shelter occupancy rates in Toronto. It represents the final installment in the LSTM series developed by the Compassionate Coders team.\n",
        "\n",
        "Across the preceding three notebooks, we've meticulously crafted four distinct LSTM implementations tailored to this project's objectives. In this concluding script, we shift our focus to rigorous testing, aiming to discern each model's performance under various conditions.\n",
        "\n",
        "Through comprehensive evaluation, we aim to identify the most effective model for forecasting homeless shelter occupancy rates. This script serves as the project's showcase, showcasing the strengths and capabilities of our LSTM implementations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW64EeaSqDDF"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MvuSdGUOlYcD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy as dc\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from geopy.geocoders import Nominatim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLroEkKKqPm-"
      },
      "source": [
        "### Dataset Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O1_GavNNqSNd"
      },
      "outputs": [],
      "source": [
        "#Occupancy Rate (Output Data):\n",
        "data_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv\"\n",
        "data_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv\"\n",
        "data_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv\"\n",
        "data_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv\"\n",
        "links = [data_24, data_23, data_22, data_21]\n",
        "\n",
        "#Weather Data\n",
        "data_w_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv\"\n",
        "data_w_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv\"\n",
        "data_w_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv\"\n",
        "data_w_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv\"\n",
        "links_weather = [data_w_24, data_w_23, data_w_22, data_w_21]\n",
        "\n",
        "#Housing\n",
        "data_housing = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv\"\n",
        "\n",
        "#Crisis helpline\n",
        "data_crisis = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bay-nW5aqt7U"
      },
      "source": [
        "class to read and report on errors when reading csv files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E6CtzOFaq2QL"
      },
      "outputs": [],
      "source": [
        "def load_csv_to_pandas(file_path):\n",
        "    try:\n",
        "        # Load CSV file into a pandas dataFrame\n",
        "        df = pd.read_csv(file_path, header=0, low_memory=False, encoding='unicode_escape')\n",
        "        print(\"Number of rows in the dataFrame:\", file_path, len(df))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCx7EX7rq7ln"
      },
      "source": [
        "This function below is to convert all the datasets of different features into a singular panda dataframes and also a hashmap containing individual shelter datas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R7R3L8Ciq78s"
      },
      "outputs": [],
      "source": [
        "def loadData(output_data, weather_data, housing, crisis):\n",
        "\n",
        "    #-------Output Data-------#\n",
        "    #Loading up the links to the output dataset\n",
        "    for i in range(len(output_data)):\n",
        "        output_data[i] = load_csv_to_pandas(output_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for output datasets\n",
        "    for i in range(len(output_data)):\n",
        "        #print(output_data[i])\n",
        "        output_data[i] = output_data[i].drop(columns = ['_id', 'ORGANIZATION_ID', 'SHELTER_ID', 'LOCATION_ID', 'LOCATION_CITY', 'LOCATION_PROVINCE', 'PROGRAM_NAME', 'SECTOR', 'PROGRAM_MODEL','OVERNIGHT_SERVICE_TYPE', 'PROGRAM_AREA', 'SERVICE_USER_COUNT', 'CAPACITY_FUNDING_BED', 'UNOCCUPIED_BEDS', 'UNAVAILABLE_BEDS', 'CAPACITY_FUNDING_ROOM', 'UNOCCUPIED_ROOMS', 'UNAVAILABLE_ROOMS'])\n",
        "        output_data[i]['OCCUPANCY_DATE'] = output_data[i]['OCCUPANCY_DATE']\n",
        "        output_data[i]['OCCUPANCY_DATE'] =  pd.to_datetime(output_data[i]['OCCUPANCY_DATE'], format='%Y-%m-%d')\n",
        "\n",
        "    #Joining the Output data together\n",
        "    big_data = output_data[0]\n",
        "    for i in range(1,len(output_data)):\n",
        "        big_data = pd.concat([big_data, output_data[i]], ignore_index = True)\n",
        "\n",
        "    #Determine the max and min date in the dataset to create a date vector to fill out empty values\n",
        "    max_date = big_data['OCCUPANCY_DATE'].max()\n",
        "    min_date = big_data['OCCUPANCY_DATE'].min()\n",
        "    date_range = pd.date_range(start=min_date, end=max_date, freq = 'D')\n",
        "    date_df = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
        "\n",
        "    #-------Weather Data-------#\n",
        "\n",
        "    #loading up the links to the weather dataset\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = load_csv_to_pandas(weather_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for weather datasets\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = weather_data[i].drop(columns = ['ï»¿\"Longitude (x)\"', 'Latitude (y)', 'Station Name', 'Climate ID', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp Flag', 'Min Temp Flag', 'Mean Temp Flag', 'Heat Deg Days Flag', 'Cool Deg Days Flag', 'Total Rain (mm)', 'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag', 'Total Precip Flag',\n",
        "        'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)', 'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)', 'Spd of Max Gust Flag'])\n",
        "        weather_data[i]['Date/Time'] = weather_data[i]['Date/Time'].astype(str)\n",
        "        weather_data[i]['Date/Time'] = pd.to_datetime(weather_data[i]['Date/Time'])\n",
        "\n",
        "    #Joining the Weather data together\n",
        "    big_weather = weather_data[0]\n",
        "    for i in range(1, len(weather_data)):\n",
        "        big_weather = pd.concat([big_weather, weather_data[i]], ignore_index = True)\n",
        "\n",
        "    #Cut down all data with dates that is bigger than the biggest date and smaller than the smallest date with an output\n",
        "    big_weather = big_weather[big_weather['Date/Time'] <= max_date]\n",
        "    big_weather = big_weather[big_weather['Date/Time'] >= min_date]\n",
        "\n",
        "    #Fill out datasets' entries w no data w 0\n",
        "    big_weather = big_weather.fillna(0)\n",
        "\n",
        "    #Changing non output dataset's date column to 'OCCUPANCY_DATE'\n",
        "    big_weather = big_weather.rename(columns = {'Date/Time': 'OCCUPANCY_DATE'})\n",
        "\n",
        "    #-------Housing Data-------#\n",
        "\n",
        "    #loading up housing data\n",
        "    housing = load_csv_to_pandas(housing)\n",
        "\n",
        "    #Dropping irrelevant columns for housing dataset\n",
        "    housing = housing[housing['GEO'] == 'Toronto, Ontario']\n",
        "    housing = housing[housing['New housing price indexes'] == 'Total (house and land)']\n",
        "    housing = housing.drop(columns = ['GEO', 'DGUID', 'New housing price indexes', 'UOM', 'UOM_ID', 'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'STATUS', 'SYMBOL', 'TERMINATED', 'DECIMALS'])\n",
        "    housing = housing.rename(columns = {housing.columns[0]: 'OCCUPANCY_DATE'})\n",
        "    housing[\"OCCUPANCY_DATE\"] = pd.to_datetime(housing[\"OCCUPANCY_DATE\"])\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] <= max_date].reset_index(drop=True)\n",
        "    housing = pd.merge(housing, date_df, on = 'OCCUPANCY_DATE', how = 'outer')\n",
        "    housing = housing.sort_values(by='OCCUPANCY_DATE').reset_index(drop=True)\n",
        "    housing = housing.ffill()\n",
        "\n",
        "    #-------Crisis Data-------#\n",
        "\n",
        "    #Loading the crisis dataset\n",
        "    crisis = load_csv_to_pandas(crisis)\n",
        "\n",
        "    #Analyize Data\n",
        "    crisis = crisis.drop(columns = ['ï»¿OBJECTID', 'EVENT_ID', 'EVENT_YEAR', 'EVENT_MONTH', 'EVENT_DOW', 'EVENT_HOUR', 'DIVISION', 'OCCURRENCE_CREATED', 'APPREHENSION_MADE', 'MCIT_ATTEND', 'HOOD_158', 'NEIGHBOURHOOD_158', 'HOOD_140', 'NEIGHBOURHOOD_140'])\n",
        "    crisis = crisis.rename(columns = {'EVENT_DATE': 'OCCUPANCY_DATE'})\n",
        "    crisis = crisis.groupby(['OCCUPANCY_DATE', 'EVENT_TYPE']).size().unstack(fill_value=0)\n",
        "    crisis.reset_index(inplace=True)\n",
        "    crisis = crisis.rename_axis(None, axis=1)\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE']).dt.date\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE'])\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] <= max_date]\n",
        "    crisis = pd.merge(date_df, crisis, on='OCCUPANCY_DATE', how='left')\n",
        "\n",
        "    #-------Final Data Prep-------#\n",
        "\n",
        "    #Merge the datasets together through date\n",
        "    big_data = pd.merge(big_data, big_weather, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, housing, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, crisis, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "\n",
        "    big_data = big_data.sort_values(by='OCCUPANCY_DATE')\n",
        "\n",
        "    #Placing the bed and room occupancy column last\n",
        "    room_occupancy = big_data.pop('OCCUPANCY_RATE_ROOMS')\n",
        "    bed_occupancy = big_data.pop('OCCUPANCY_RATE_BEDS')\n",
        "    big_data['OCCUPANCY_RATE_BEDS'] = bed_occupancy\n",
        "    big_data['OCCUPANCY_RATE_ROOMS'] = room_occupancy\n",
        "\n",
        "    grouped_data = big_data.groupby('PROGRAM_ID')\n",
        "    shelter_data_frames = {}\n",
        "    for shelter_id, shelter_group in grouped_data:\n",
        "        shelter_data_frames[shelter_id] = shelter_group\n",
        "        shelter_data_frames[shelter_id]['OCCUPANCY_DATE'] = pd.to_datetime(shelter_data_frames[shelter_id]['OCCUPANCY_DATE'])\n",
        "\n",
        "    big_data.reset_index(inplace=True)\n",
        "    big_data = big_data.drop(columns = ['index'])\n",
        "\n",
        "    return big_data, shelter_data_frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O74CRlk5dtZ"
      },
      "source": [
        "The dataframe above contains every single data points of every shelters from 2021 to 2024. Inorder to get a trainable dataset for the combined shelter data multivariate model, we will have to combine the data of everysingle shelters together into one big city wide occupancy rates. This function below will achieve that target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yblOfdLx5iR4"
      },
      "outputs": [],
      "source": [
        "def merge_Shelters_Data(df):\n",
        "\n",
        "    df = df.drop(columns = ['ORGANIZATION_NAME', 'SHELTER_GROUP', 'LOCATION_NAME', 'LOCATION_ADDRESS', 'LOCATION_POSTAL_CODE', 'PROGRAM_ID', 'CAPACITY_TYPE', 'OCCUPANCY_RATE_BEDS', 'OCCUPANCY_RATE_ROOMS'])\n",
        "    grouped_capacity = df.groupby('OCCUPANCY_DATE')[['CAPACITY_ACTUAL_BED', 'CAPACITY_ACTUAL_ROOM']].sum()\n",
        "    grouped_occupied = df.groupby('OCCUPANCY_DATE')[['OCCUPIED_BEDS', 'OCCUPIED_ROOMS']].sum()\n",
        "\n",
        "    df = df.merge(grouped_capacity, on='OCCUPANCY_DATE', suffixes=('', '_TOTAL_CAPACITY'))\n",
        "    df = df.merge(grouped_occupied, on='OCCUPANCY_DATE', suffixes=('', '_TOTAL_OCCUPIED'))\n",
        "    df = df.drop(columns = ['CAPACITY_ACTUAL_BED', 'CAPACITY_ACTUAL_ROOM', 'OCCUPIED_BEDS', 'OCCUPIED_ROOMS'])\n",
        "    df = df.drop_duplicates()\n",
        "\n",
        "    df['TOTAL_OCCUPIED'] = df['OCCUPIED_BEDS_TOTAL_OCCUPIED'] + df['OCCUPIED_ROOMS_TOTAL_OCCUPIED']\n",
        "    df['TOTAL_CAPACITY'] = df['CAPACITY_ACTUAL_BED_TOTAL_CAPACITY'] + df['CAPACITY_ACTUAL_ROOM_TOTAL_CAPACITY']\n",
        "    df['OCCUPIED_PERCENTAGE'] = 100 * df['TOTAL_OCCUPIED']/df['TOTAL_CAPACITY']\n",
        "    df = df.drop(columns = ['CAPACITY_ACTUAL_BED_TOTAL_CAPACITY', 'CAPACITY_ACTUAL_ROOM_TOTAL_CAPACITY', 'OCCUPIED_BEDS_TOTAL_OCCUPIED', 'OCCUPIED_ROOMS_TOTAL_OCCUPIED', 'TOTAL_CAPACITY', 'TOTAL_OCCUPIED'])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmFYTI_mq_8o"
      },
      "source": [
        "Running the function to get the dataframe and hashmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0wGfbAtrBQ4",
        "outputId": "daad938f-dd56-44d5-f690-0b278ec8edc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv 11459\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv 48345\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv 49478\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv 50944\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv 366\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv 62160\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv 291991\n"
          ]
        }
      ],
      "source": [
        "dataframe, iso_data = loadData(links.copy(), links_weather.copy(), data_housing, data_crisis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfYI4d4DtpwI"
      },
      "source": [
        "### Convert Dataset to DataLoader\n",
        "\n",
        "With the proper dataframe, we will need to convert the dataset into a timeseries and then dataloader to feed into the model for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7W-X2jAua47"
      },
      "source": [
        "This function below is to grab the scaler to scale the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_cZ1jYCZubtx"
      },
      "outputs": [],
      "source": [
        "def get_scaler():\n",
        "    scaler = MinMaxScaler(feature_range = (-1, 1))\n",
        "    return scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7D_eJhTueP6"
      },
      "source": [
        "Differing than the Univariate, we will have to check if all features have all of their values filled in. If not we must manually fill it in by taking the average of the existing data. The function below accomplishes that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KLuzefFruVSt"
      },
      "outputs": [],
      "source": [
        "def feature_check(df):\n",
        "    df_ = dc(df)\n",
        "    for i in df_.columns:\n",
        "        if df_[i].isna().any():\n",
        "            avg = df_[i].mean()\n",
        "            df_.loc[:, i] = df_[i].fillna(avg)\n",
        "    return df_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75S3LkVzujUa"
      },
      "source": [
        "To Convert the dataset into a dataloader, we will need to define a Dataset class for the time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Vat87lPZukc2"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgUNtpDnumvV"
      },
      "source": [
        "This function below will convert the dataframe to a time series for the model with proper hyperparameters as inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kWTCC2ehupPP"
      },
      "outputs": [],
      "source": [
        "def time_series_converter(iso_data, scaler, n_past, n_future, train_test_split, batch_size, used_features, shel_group = None):\n",
        "\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "\n",
        "    if shel_group is not None:\n",
        "\n",
        "        one_hot_len = max(shel_group.values()) + 1\n",
        "\n",
        "        num_feat = len(used_features) - 1 + one_hot_len\n",
        "\n",
        "        dfs = []\n",
        "\n",
        "        #Iterate through all useable Shelters\n",
        "        for i in shel_group:\n",
        "\n",
        "            #Getting the df from Iso Data\n",
        "            df = dc(iso_data[int(i)])\n",
        "\n",
        "            #Unifying the df to have the same output column name\n",
        "            if df['OCCUPANCY_RATE_ROOMS'].isna().all():\n",
        "                df = df.rename(columns = {'OCCUPANCY_RATE_BEDS': 'OCCUPIED_PERCENTAGE'})\n",
        "            else:\n",
        "                df = df.rename(columns = {'OCCUPANCY_RATE_ROOMS': 'OCCUPIED_PERCENTAGE'})\n",
        "\n",
        "            df = df[used_features]\n",
        "\n",
        "            df = feature_check(df)\n",
        "\n",
        "            for z in range(one_hot_len):\n",
        "                if shel_group[i] == z:\n",
        "                    df['Feature_' + str(z)] = 1\n",
        "                else:\n",
        "                    df['Feature_' + str(z)] = 0\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "            #Concatenating all Dfs together\n",
        "            concatenated_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "            #Isolate the feature columns\n",
        "            iso_col = concatenated_df[['Feature_' + str(i) for i in range(one_hot_len)]]\n",
        "\n",
        "            #Scaled the dfs\n",
        "            scaler = scaler.fit(concatenated_df[used_features[1:]])\n",
        "            np_df = scaler.fit_transform(concatenated_df[used_features[1:]])\n",
        "            df_scaled = pd.DataFrame(np_df, columns=used_features[1:])\n",
        "\n",
        "            #Combined the final df together\n",
        "            np_df = pd.concat([iso_col, df_scaled], axis=1).values\n",
        "\n",
        "    else:\n",
        "        df = dc(iso_data)\n",
        "        df.set_index('OCCUPANCY_DATE', inplace=True)\n",
        "        df = df.astype(float)\n",
        "        scaler = scaler.fit(df)\n",
        "\n",
        "        np_df = scaler.transform(df)\n",
        "\n",
        "        num_feat = len([i for i in df])\n",
        "\n",
        "    #Converting it into a time series\n",
        "    for i in range(n_past, len(np_df) - n_future + 1):\n",
        "        train_x.append(np_df[i - n_past: i, 0:np_df.shape[1]])\n",
        "        train_y.append(np_df[i: i + n_future, - 1])\n",
        "\n",
        "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
        "\n",
        "    split_index = int(len(train_x) * train_test_split)\n",
        "\n",
        "    X_train = train_x[:split_index]\n",
        "    X_test = train_x[split_index:]\n",
        "\n",
        "    Y_train = train_y[:split_index]\n",
        "    Y_test = train_y[split_index:]\n",
        "\n",
        "    X_train_ = X_train.reshape((-1, n_past, num_feat))\n",
        "    X_test_ = X_test.reshape((-1, n_past, num_feat))\n",
        "\n",
        "    X_train = torch.tensor(X_train).float()\n",
        "    Y_train = torch.tensor(Y_train).float()\n",
        "    X_test = torch.tensor(X_test).float()\n",
        "    Y_test = torch.tensor(Y_test).float()\n",
        "\n",
        "    train_Dataset = TimeSeriesDataset(X_train, Y_train)\n",
        "    test_Dataset = TimeSeriesDataset(X_test, Y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_Dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = DataLoader(test_Dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnfWQOGQvB6p"
      },
      "source": [
        "### Define the Model\n",
        "\n",
        "Before training can begin, we will have to define the model architecture. Since the model is an lstm, it will contain LSTM architectures and also fully connected layers for the final output result. Please see the infrastructure below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ahjh8mXUvGT2"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size) #output_size being how many days in the future to predict\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7aCZA5pvIqg"
      },
      "source": [
        "Model Training Function. Call this function and pass in the model to begin training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aypqC8DKvQH8"
      },
      "outputs": [],
      "source": [
        "def begin_training(model,num_epochs, train_loader, test_loader, loss, optimizer):\n",
        "\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    average_validation_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        #Training\n",
        "        model.train(True)\n",
        "        running_loss = 0\n",
        "        if epoch % 5 == 0:\n",
        "          print(\"Epoch: \" + str(epoch))\n",
        "\n",
        "        for batch_index, batch in enumerate(train_loader):\n",
        "            x_batch, y_batch = batch[0], batch[1]\n",
        "            output = model(x_batch)\n",
        "            loss_ = loss(output, y_batch)\n",
        "            running_loss += loss_.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "          print(\"Training Loss: \" + str(running_loss))\n",
        "        training_loss.append(running_loss)\n",
        "\n",
        "        #Validating\n",
        "        model.train(False)\n",
        "        vad_loss = 0\n",
        "\n",
        "        for batch_index, batch in enumerate(test_loader):\n",
        "            x_batch, y_batch = batch[0], batch[1]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output    = model(x_batch)\n",
        "                loss_     = loss(output, y_batch)\n",
        "                vad_loss += loss_.item()\n",
        "\n",
        "        validation_loss.append(vad_loss)\n",
        "        avg_loss_across_batches = vad_loss / len(test_loader)\n",
        "        average_validation_loss.append(avg_loss_across_batches)\n",
        "        if epoch % 5 == 0:\n",
        "          print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
        "          print('***************************************************')\n",
        "          print('\\n')\n",
        "\n",
        "\n",
        "    return model, training_loss, validation_loss, average_validation_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmjnSNnK9iVn"
      },
      "source": [
        "This function below randomized the weights of the model. This function is to make sure the model is properly initialized while testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l2vkIN4J9ole"
      },
      "outputs": [],
      "source": [
        "def reset_weights(module):\n",
        "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        module.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWZHq4VuurGe"
      },
      "source": [
        "### Testing Commences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBJ9qqeWAE7C"
      },
      "source": [
        "Since the multivariate models that uses one hot encoded grouping are trained on only 62 viable shelters, the rest of the shelters will be eliminated from testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K_n6TFkAD8vE"
      },
      "outputs": [],
      "source": [
        "viable_shelters = [11794, 11798, 11799, 11815, 11831, 11871, 11891, 11895, 11911, 11971, 12011, 12053, 12231, 12251, 12252, 12254, 12274, 12291, 12292, 12471, 12711, 13451, 13932, 14051, 14251, 14571, 14572, 14631, 14651, 14671, 14931, 15111, 15112, 15171, 15711, 15811, 15871, 16111, 16131, 16151, 16191, 16192, 16193, 16194, 16271, 16311, 16371, 16671, 16691, 16891, 16892, 16911, 17011, 17012, 17191, 17211, 17212, 17691, 17771, 17772, 17791, 17811]\n",
        "for i in iso_data.copy():\n",
        "  if i not in viable_shelters:\n",
        "    del iso_data[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing purposes, we will only be comparing our accuracy with these 5 shelters: 11794, 11831, 11252, 11254, 11274"
      ],
      "metadata": {
        "id": "d-Jk8Y_ca6rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_shelters = [11794, 11831, 12252, 12254, 12274]\n",
        "test_data = iso_data.copy()\n",
        "for i in test_data.copy():\n",
        "  if i not in test_shelters:\n",
        "    del test_data[i]"
      ],
      "metadata": {
        "id": "ATO0xdPrbA_4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6fnFRFaPW96"
      },
      "source": [
        "We will also need to use the model to infer future dates to calculate their performance. The function below achieves that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Fxj8sODaPebl"
      },
      "outputs": [],
      "source": [
        "def infer_date_(model, df, scaler, n_future, future_days = None, one_hot_feature = None):\n",
        "\n",
        "    #Set model training to False\n",
        "    model.train(False)\n",
        "    copy_df = dc(df)\n",
        "\n",
        "    #Case 1: if model predicts multiple days at once\n",
        "    if n_future > 1:\n",
        "\n",
        "        #Build Dateframe for future days\n",
        "        max_date = df['OCCUPANCY_DATE'].max()\n",
        "        date_range = pd.date_range(start=max_date, end=max_date + pd.Timedelta(days=n_future), freq = 'D')\n",
        "        df_new = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
        "\n",
        "        #Scaling input Dataframe\n",
        "        copy_df.set_index('OCCUPANCY_DATE', inplace = True)\n",
        "        df_scaled = scaler.fit_transform(copy_df)\n",
        "\n",
        "        if one_hot_feature is not None:\n",
        "            for i in range(one_hot_feature[1]):\n",
        "\n",
        "                if i != one_hot_feature[0]:\n",
        "                    arr = np.zeros((df_scaled.shape[0], 1))\n",
        "                    df_scaled = np.hstack((arr, df_scaled))\n",
        "                else:\n",
        "                    arr = np.ones((df_scaled.shape[0], 1))\n",
        "                    df_scaled = np.hstack((arr, df_scaled))\n",
        "\n",
        "        #Convert data to tensor and passing it into the model to get predicted data and converting it into a panda dataframe before returning it\n",
        "        y = model(torch.tensor(df_scaled).unsqueeze(0).float()).detach().numpy().reshape(-1,1)\n",
        "        y_ = np.repeat(y, copy_df.shape[1], axis = -1)\n",
        "        y_actual = scaler.inverse_transform(y_)[:,-1]\n",
        "        y_inserted = np.insert(y_actual, 0, df['OCCUPIED_PERCENTAGE'].iloc[-1])\n",
        "        df_new['OCCUPIED_PERCENTAGE'] = pd.DataFrame(y_inserted, columns = ['OCCUPIED_PERCENTAGE'])\n",
        "\n",
        "\n",
        "    #Case 2: if model predicts one day at a time; therefore need loop to predict all future_days days.\n",
        "    if n_future == 1 and future_days is not None:\n",
        "\n",
        "        data = torch.tensor(scaler.fit_transform(np.array(copy_df['OCCUPIED_PERCENTAGE']).reshape(-1, 1)).reshape((-1, copy_df.shape[0], 1))).float()\n",
        "        for i in range(future_days):\n",
        "            y = model(data).unsqueeze(0)\n",
        "            data = torch.cat((data, y), dim = 1)\n",
        "        data = scaler.inverse_transform(data.squeeze().detach().numpy().reshape(-1, 1)).flatten()\n",
        "\n",
        "        #Adding a data column to the new data\n",
        "        max_date = copy_df['OCCUPANCY_DATE'].max()\n",
        "        date_range = pd.date_range(start=max_date , end=max_date + pd.Timedelta(days=future_days), freq = 'D')\n",
        "        df_new = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
        "\n",
        "        #Getting the newly generated portion of data\n",
        "        new_data = data[-future_days:]\n",
        "        new_data = np.insert(new_data, 0, copy_df['OCCUPIED_PERCENTAGE'].iloc[-1])\n",
        "        new_data_df = pd.DataFrame(new_data, columns = ['OCCUPIED_PERCENTAGE'])\n",
        "\n",
        "        #Combined\n",
        "        df_new['OCCUPIED_PERCENTAGE'] = new_data_df\n",
        "\n",
        "    return df_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTcMZEbhO1cL"
      },
      "source": [
        "For each feature set that we test on, we will ask each model implementation to infer data for every single viable individual shelter and calculate the average loss for each model. The function below will take the model and iterate through all individual shelter to find the average loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7fAgMwhGPHog"
      },
      "outputs": [],
      "source": [
        "def calc_loss(model, iso_data, used_features, shel_grouping = None):\n",
        "  #Initialize the Loss variables\n",
        "  total_shel = 0\n",
        "  loss_ = 0\n",
        "\n",
        "  total_shel_ = 0\n",
        "  loss_sq = 0\n",
        "\n",
        "  for i in iso_data:\n",
        "\n",
        "    if len(iso_data[i]) > 60:\n",
        "      df = iso_data[i]\n",
        "\n",
        "      #Rename the columns\n",
        "      if df['OCCUPANCY_RATE_ROOMS'].isna().all():\n",
        "        df = df.rename(columns = {'OCCUPANCY_RATE_BEDS': 'OCCUPIED_PERCENTAGE'})\n",
        "      else:\n",
        "        df = df.rename(columns = {'OCCUPANCY_RATE_ROOMS': 'OCCUPIED_PERCENTAGE'})\n",
        "\n",
        "      #Selecting on used features\n",
        "      df = df[used_features]\n",
        "\n",
        "      df = feature_check(df)\n",
        "\n",
        "      #Move the dates back by 60 days\n",
        "      df_use_infer = dc(df)\n",
        "      use_date = max(df_use_infer['OCCUPANCY_DATE']) - pd.Timedelta(days = 60)\n",
        "      df_use_infer = df_use_infer[df_use_infer['OCCUPANCY_DATE'] <= use_date]\n",
        "\n",
        "      #Inferring Data\n",
        "      if shel_grouping is None:\n",
        "        df_infer = infer_date_(model, df_use_infer, scaler, n_future, future_days = 60, one_hot_feature = None)\n",
        "        df_infer_date = min(df_infer['OCCUPANCY_DATE'])\n",
        "        df_infer = df_infer[df_infer['OCCUPANCY_DATE'] > df_infer_date]\n",
        "      else:\n",
        "        df_infer = infer_date_(model, df_use_infer, scaler, n_future, future_days = 60, one_hot_feature = [shel_grouping[i], max(shel_grouping.values()) + 1])\n",
        "        df_infer_date = min(df_infer['OCCUPANCY_DATE'])\n",
        "        df_infer = df_infer[df_infer['OCCUPANCY_DATE'] > df_infer_date]\n",
        "\n",
        "      #data used for loss calculations\n",
        "      df_loss = dc(df)\n",
        "      df_loss = df_loss[df_loss['OCCUPANCY_DATE'] > use_date]\n",
        "\n",
        "      #Calculating Loss\n",
        "      df_np = df_loss['OCCUPIED_PERCENTAGE'].values\n",
        "      df_np_infer = df_infer['OCCUPIED_PERCENTAGE'].values\n",
        "\n",
        "      temp_loss = 0\n",
        "      temp_loss_sq = 0\n",
        "      for i in range(len(df_np)):\n",
        "        temp_loss += abs(df_np[i] - df_np_infer[i])\n",
        "        temp_loss_sq += abs(df_np[i] - df_np_infer[i]) ** 2\n",
        "      if temp_loss > 0:\n",
        "        total_shel += 1\n",
        "        loss_ += temp_loss\n",
        "      if temp_loss_sq > 0:\n",
        "        total_shel_ += 1\n",
        "        loss_sq += temp_loss_sq\n",
        "      loss_ /= 60\n",
        "      loss_sq /= 60\n",
        "      loss_sq = loss_sq ** (1/2)\n",
        "\n",
        "\n",
        "  avg_loss = loss_/total_shel\n",
        "  avg_loss_sq = loss_sq/total_shel_\n",
        "  return avg_loss, avg_loss_sq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qxxsdsec3cqx"
      },
      "source": [
        "Define Model's Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rroXOZGZ3kmK"
      },
      "outputs": [],
      "source": [
        "n_steps = 90\n",
        "n_future = 60\n",
        "batch_size = 16\n",
        "train_test_split = 0.75\n",
        "scaler = get_scaler()\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 20\n",
        "train_test_split = 0.8\n",
        "loss_function = nn.MSELoss()\n",
        "hidden_size = 120\n",
        "num_stacked_layers = 1\n",
        "output_size = n_future"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnKBrb9nuL4W"
      },
      "source": [
        "Extract a list of all trainable features for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P30BMyjVo2H",
        "outputId": "072876d8-9a2d-43cf-bed5-8fac3890b05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of all trainable Features:\n",
            "['Max Temp (Â°C)', 'Min Temp (Â°C)', 'Mean Temp (Â°C)', 'Heat Deg Days (Â°C)', 'Cool Deg Days (Â°C)', 'Total Precip (mm)', 'Snow on Grnd (cm)', 'VALUE', 'Overdose', 'Person in Crisis', 'Suicide-related']\n"
          ]
        }
      ],
      "source": [
        "#Obtain all features\n",
        "total_features = []\n",
        "for i in dataframe:\n",
        "  total_features.append(i)\n",
        "\n",
        "#Remove shelter identification features and keeping only trainable features\n",
        "untrainable_features = ['OCCUPANCY_DATE', 'ORGANIZATION_NAME', 'SHELTER_GROUP', 'LOCATION_NAME', 'LOCATION_ADDRESS', 'LOCATION_POSTAL_CODE', 'PROGRAM_ID', 'CAPACITY_TYPE', 'CAPACITY_ACTUAL_BED', 'OCCUPIED_BEDS', 'CAPACITY_ACTUAL_ROOM', 'OCCUPIED_ROOMS', 'OCCUPANCY_RATE_BEDS', 'OCCUPANCY_RATE_ROOMS']\n",
        "for i in untrainable_features:\n",
        "  total_features.remove(i)\n",
        "\n",
        "print(\"List of all trainable Features:\")\n",
        "print(total_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7U71Wj709Aj"
      },
      "source": [
        "Loop to iterate through every combinations of trainable features inorder to test their effect on different Model's implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs04rzrJ0x_m"
      },
      "outputs": [],
      "source": [
        "#Hashmap to store the result of each model with each feature set\n",
        "result_map = {}\n",
        "\n",
        "# Loop through all possible combinations of elements in the list\n",
        "for r in range(1, len(total_features) + 1):\n",
        "    for features_used in combinations(total_features, r):\n",
        "\n",
        "        #Combine feature_used with date and output feature\n",
        "        features_used = list(features_used)\n",
        "        features_used.insert(0, 'OCCUPANCY_DATE')\n",
        "        features_used.append('OCCUPIED_PERCENTAGE')\n",
        "\n",
        "        #Init the hashmap's key\n",
        "        result_map[tuple(features_used)] = {'Combined': [], 'Distortions': [], 'Correlation': []}\n",
        "\n",
        "        #Build the dataframe\n",
        "        df = feature_check(merge_Shelters_Data(dc(dataframe))[features_used])\n",
        "\n",
        "        #----------------Combined Shelter Multivariate Model----------------#\n",
        "\n",
        "        #Converting the dataframe into dataloader\n",
        "        combined_train, combined_test = time_series_converter(dc(df), scaler, n_steps, n_future, train_test_split, batch_size, features_used, shel_group = None)\n",
        "\n",
        "        #Define the model\n",
        "        input_size = len(features_used) - 1\n",
        "        model_combined = LSTM(input_size, hidden_size, num_stacked_layers, output_size)\n",
        "        optimizer_combined = torch.optim.AdamW(model_combined.parameters(), lr = learning_rate)\n",
        "\n",
        "        #Zeroing out the optimizer and randomized the weights of the model\n",
        "        optimizer_combined.zero_grad()\n",
        "        reset_weights(model_combined)\n",
        "\n",
        "        #Begin training\n",
        "        #model_combined, training_loss, valid_loss, avg_valid_loss = begin_training(model_combined, num_epochs, combined_train, combined_test, loss_function, optimizer_combined)\n",
        "\n",
        "        #Calculate loss\n",
        "        comb_loss, comb_loss_sq = calc_loss(model_combined, iso_data, features_used)\n",
        "        #result_map[tuple(features_used)]['Combined'].append(comb_loss)\n",
        "\n",
        "        #----------------Distortion Location Grouping Multivariate Model----------------#\n",
        "        distortion_grouping = {11794: 2, 11798: 2, 11799: 2, 11815: 3, 11831: 2, 11871: 2, 11891: 2, 11895: 2, 11911: 1, 11971: 2, 12011: 2, 12053: 2, 12231: 2, 12251: 2, 12252: 0, 12254: 3, 12274: 1, 12291: 0, 12292: 0, 12471: 2, 12711: 2, 13451: 2, 13932: 1, 14051: 3, 14251: 2, 14571: 0, 14572: 0, 14631: 2, 14651: 2, 14671: 2, 14931: 2, 15111: 1, 15112: 1, 15171: 2, 15711: 2, 15811: 3, 15871: 3, 16111: 0, 16131: 2, 16151: 2, 16191: 3, 16192: 3, 16193: 3, 16194: 1, 16271: 3, 16311: 3, 16371: 2, 16671: 3, 16691: 3, 16891: 0, 16892: 0, 16911: 1, 17011: 3, 17012: 3, 17191: 1, 17211: 3, 17212: 3, 17691: 1, 17771: 1, 17772: 1, 17791: 1, 17811: 1}\n",
        "\n",
        "        #Converting the dataframe into dataloader\n",
        "        dist_train, dist_test = time_series_converter(iso_data.copy(), scaler, n_steps, n_future, train_test_split, batch_size, features_used, distortion_grouping)\n",
        "\n",
        "        #Define the Model\n",
        "        input_size = len(features_used) + max(distortion_grouping.values())\n",
        "        model_dist = LSTM(input_size, hidden_size, num_stacked_layers, output_size)\n",
        "        optimizer_dist = torch.optim.AdamW(model_dist.parameters(), lr = learning_rate)\n",
        "\n",
        "        #Zeroing out the optimizer and randomized the weights of the model\n",
        "        optimizer_dist.zero_grad()\n",
        "        reset_weights(model_dist)\n",
        "\n",
        "        #Begin training\n",
        "        #model_dist, training_loss, valid_loss, avg_valid_loss = begin_training(model_dist, num_epochs, dist_train, dist_test, loss_function, optimizer_dist)\n",
        "\n",
        "        #Calculate loss\n",
        "        dist_loss, dist_loss_sq = calc_loss(model_dist, iso_data, features_used, distortion_grouping)\n",
        "        #result_map[tuple(features_used)]['Distortions'].append(dist_loss)\n",
        "\n",
        "        #----------------Correlation Location Grouping Multivariate Model----------------#\n",
        "        correlation_grouping = {11794: 3, 11798: 1, 11799: 1, 11815: 0, 11831: 1, 11871: 3, 11891: 1, 11895: 1, 11911: 7, 11971: 3, 12011: 2, 12053: 2, 12231: 2, 12251: 1, 12252: 5, 12254: 0, 12274: 7, 12291: 6, 12292: 6, 12471: 1, 12711: 1, 13451: 3, 13932: 7, 14051: 6, 14251: 1, 14571: 6, 14572: 6, 14631: 1, 14651: 3, 14671: 3, 14931: 1, 15111: 7, 15112: 7, 15171: 2, 15711: 2, 15811: 6, 15871: 6, 16111: 6, 16131: 1, 16151: 1, 16191: 4, 16192: 4, 16193: 4, 16194: 7, 16271: 4, 16311: 4, 16371: 3, 16671: 4, 16691: 4, 16891: 4, 16892: 4, 16911: 7, 17011: 4, 17012: 4, 17191: 7, 17211: 6, 17212: 6, 17691: 7, 17771: 7, 17772: 7, 17791: 7, 17811: 7}\n",
        "\n",
        "        #Converting the dataframe into dataloader\n",
        "        corr_train, corr_test = time_series_converter(iso_data.copy(), scaler, n_steps, n_future, train_test_split, batch_size, features_used, correlation_grouping)\n",
        "\n",
        "        #Define the Model\n",
        "        input_size = len(features_used) + max(correlation_grouping.values())\n",
        "        model_corr = LSTM(input_size, hidden_size, num_stacked_layers, output_size)\n",
        "        optimizer_corr = torch.optim.AdamW(model_corr.parameters(), lr = learning_rate)\n",
        "\n",
        "        #Zeroing out the optimizer and randomized the weights of the model\n",
        "        optimizer_corr.zero_grad()\n",
        "        reset_weights(model_corr)\n",
        "\n",
        "        #Begin training\n",
        "        #model_corr, training_loss, valid_loss, avg_valid_loss = begin_training(model_corr, num_epochs, corr_train, corr_test, loss_function, optimizer_corr)\n",
        "\n",
        "        #Calculate loss\n",
        "        corr_loss, corr_loss_sq = calc_loss(model_corr, iso_data, features_used, correlation_grouping)\n",
        "        #result_map[tuple(features_used)]['Correlation'].append(corr_loss)\n",
        "\n",
        "        print(\"Results for Feature set: \" + str(features_used))\n",
        "        #print('Combined Model: ' + str(round(result_map[tuple(features_used)]['Combined'][0], 2)) + ' | Distortion Model: ' + str(round(result_map[tuple(features_used)]['Distortions'][0],2)) + ' | Correlation Grouping Model: ' + str(round(result_map[tuple(features_used)]['Correlation'][0],2)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}