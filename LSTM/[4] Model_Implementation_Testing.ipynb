{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation Testing"
      ],
      "metadata": {
        "id": "iGQy35rUlcRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Nida Copty, India Tory, Emily Nguyen, Thomas Nguyen"
      ],
      "metadata": {
        "id": "ejxryfnymb4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook marks the culmination of our efforts in constructing LSTM models to forecast homeless shelter occupancy rates in Toronto. It represents the final installment in the LSTM series developed by the Compassionate Coders team.\n",
        "\n",
        "Across the preceding three notebooks, we've meticulously crafted four distinct LSTM implementations tailored to this project's objectives. In this concluding script, we shift our focus to rigorous testing, aiming to discern each model's performance under various conditions.\n",
        "\n",
        "Through comprehensive evaluation, we aim to identify the most effective model for forecasting homeless shelter occupancy rates. This script serves as the project's showcase, showcasing the strengths and capabilities of our LSTM implementations."
      ],
      "metadata": {
        "id": "nja30yAXm15f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "aW64EeaSqDDF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MvuSdGUOlYcD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy as dc\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from geopy.geocoders import Nominatim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Load"
      ],
      "metadata": {
        "id": "MLroEkKKqPm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Occupancy Rate (Output Data):\n",
        "data_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv\"\n",
        "data_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv\"\n",
        "data_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv\"\n",
        "data_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv\"\n",
        "links = [data_24, data_23, data_22, data_21]\n",
        "\n",
        "#Weather Data\n",
        "data_w_23 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv\"\n",
        "data_w_24 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv\"\n",
        "data_w_22 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv\"\n",
        "data_w_21 = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv\"\n",
        "links_weather = [data_w_24, data_w_23, data_w_22, data_w_21]\n",
        "\n",
        "#Housing\n",
        "data_housing = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv\"\n",
        "\n",
        "#Crisis helpline\n",
        "data_crisis = r\"/content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv\""
      ],
      "metadata": {
        "id": "O1_GavNNqSNd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class to read and report on errors when reading csv files."
      ],
      "metadata": {
        "id": "bay-nW5aqt7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv_to_pandas(file_path):\n",
        "    try:\n",
        "        # Load CSV file into a pandas dataFrame\n",
        "        df = pd.read_csv(file_path, header=0, low_memory=False, encoding='unicode_escape')\n",
        "        print(\"Number of rows in the dataFrame:\", file_path, len(df))\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None"
      ],
      "metadata": {
        "id": "E6CtzOFaq2QL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function below is to convert all the datasets of different features into a singular panda dataframes and also a hashmap containing individual shelter datas."
      ],
      "metadata": {
        "id": "lCx7EX7rq7ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(output_data, weather_data, housing, crisis):\n",
        "\n",
        "    #-------Output Data-------#\n",
        "    #Loading up the links to the output dataset\n",
        "    for i in range(len(output_data)):\n",
        "        output_data[i] = load_csv_to_pandas(output_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for output datasets\n",
        "    for i in range(len(output_data)):\n",
        "        #print(output_data[i])\n",
        "        output_data[i] = output_data[i].drop(columns = ['_id', 'ORGANIZATION_ID', 'SHELTER_ID', 'LOCATION_ID', 'LOCATION_CITY', 'LOCATION_PROVINCE', 'PROGRAM_NAME', 'SECTOR', 'PROGRAM_MODEL','OVERNIGHT_SERVICE_TYPE', 'PROGRAM_AREA', 'SERVICE_USER_COUNT', 'CAPACITY_FUNDING_BED', 'UNOCCUPIED_BEDS', 'UNAVAILABLE_BEDS', 'CAPACITY_FUNDING_ROOM', 'UNOCCUPIED_ROOMS', 'UNAVAILABLE_ROOMS'])\n",
        "        output_data[i]['OCCUPANCY_DATE'] = output_data[i]['OCCUPANCY_DATE']\n",
        "        output_data[i]['OCCUPANCY_DATE'] =  pd.to_datetime(output_data[i]['OCCUPANCY_DATE'], format='%Y-%m-%d')\n",
        "\n",
        "    #Joining the Output data together\n",
        "    big_data = output_data[0]\n",
        "    for i in range(1,len(output_data)):\n",
        "        big_data = pd.concat([big_data, output_data[i]], ignore_index = True)\n",
        "\n",
        "    #Determine the max and min date in the dataset to create a date vector to fill out empty values\n",
        "    max_date = big_data['OCCUPANCY_DATE'].max()\n",
        "    min_date = big_data['OCCUPANCY_DATE'].min()\n",
        "    date_range = pd.date_range(start=min_date, end=max_date, freq = 'D')\n",
        "    date_df = pd.DataFrame({'OCCUPANCY_DATE': date_range})\n",
        "\n",
        "    #-------Weather Data-------#\n",
        "\n",
        "    #loading up the links to the weather dataset\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = load_csv_to_pandas(weather_data[i])\n",
        "\n",
        "    #Dropping irrelevant columns for weather datasets\n",
        "    for i in range(len(weather_data)):\n",
        "        weather_data[i] = weather_data[i].drop(columns = ['ï»¿\"Longitude (x)\"', 'Latitude (y)', 'Station Name', 'Climate ID', 'Year', 'Month', 'Day', 'Data Quality', 'Max Temp Flag', 'Min Temp Flag', 'Mean Temp Flag', 'Heat Deg Days Flag', 'Cool Deg Days Flag', 'Total Rain (mm)', 'Total Rain Flag', 'Total Snow (cm)', 'Total Snow Flag', 'Total Precip Flag',\n",
        "        'Snow on Grnd Flag', 'Dir of Max Gust (10s deg)', 'Dir of Max Gust Flag', 'Spd of Max Gust (km/h)', 'Spd of Max Gust Flag'])\n",
        "        weather_data[i]['Date/Time'] = weather_data[i]['Date/Time'].astype(str)\n",
        "        weather_data[i]['Date/Time'] = pd.to_datetime(weather_data[i]['Date/Time'])\n",
        "\n",
        "    #Joining the Weather data together\n",
        "    big_weather = weather_data[0]\n",
        "    for i in range(1, len(weather_data)):\n",
        "        big_weather = pd.concat([big_weather, weather_data[i]], ignore_index = True)\n",
        "\n",
        "    #Cut down all data with dates that is bigger than the biggest date and smaller than the smallest date with an output\n",
        "    big_weather = big_weather[big_weather['Date/Time'] <= max_date]\n",
        "    big_weather = big_weather[big_weather['Date/Time'] >= min_date]\n",
        "\n",
        "    #Fill out datasets' entries w no data w 0\n",
        "    big_weather = big_weather.fillna(0)\n",
        "\n",
        "    #Changing non output dataset's date column to 'OCCUPANCY_DATE'\n",
        "    big_weather = big_weather.rename(columns = {'Date/Time': 'OCCUPANCY_DATE'})\n",
        "\n",
        "    #-------Housing Data-------#\n",
        "\n",
        "    #loading up housing data\n",
        "    housing = load_csv_to_pandas(housing)\n",
        "\n",
        "    #Dropping irrelevant columns for housing dataset\n",
        "    housing = housing[housing['GEO'] == 'Toronto, Ontario']\n",
        "    housing = housing[housing['New housing price indexes'] == 'Total (house and land)']\n",
        "    housing = housing.drop(columns = ['GEO', 'DGUID', 'New housing price indexes', 'UOM', 'UOM_ID', 'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'STATUS', 'SYMBOL', 'TERMINATED', 'DECIMALS'])\n",
        "    housing = housing.rename(columns = {housing.columns[0]: 'OCCUPANCY_DATE'})\n",
        "    housing[\"OCCUPANCY_DATE\"] = pd.to_datetime(housing[\"OCCUPANCY_DATE\"])\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    housing = housing[housing[\"OCCUPANCY_DATE\"] <= max_date].reset_index(drop=True)\n",
        "    housing = pd.merge(housing, date_df, on = 'OCCUPANCY_DATE', how = 'outer')\n",
        "    housing = housing.sort_values(by='OCCUPANCY_DATE').reset_index(drop=True)\n",
        "    housing = housing.ffill()\n",
        "\n",
        "    #-------Crisis Data-------#\n",
        "\n",
        "    #Loading the crisis dataset\n",
        "    crisis = load_csv_to_pandas(crisis)\n",
        "\n",
        "    #Analyize Data\n",
        "    crisis = crisis.drop(columns = ['ï»¿OBJECTID', 'EVENT_ID', 'EVENT_YEAR', 'EVENT_MONTH', 'EVENT_DOW', 'EVENT_HOUR', 'DIVISION', 'OCCURRENCE_CREATED', 'APPREHENSION_MADE', 'MCIT_ATTEND', 'HOOD_158', 'NEIGHBOURHOOD_158', 'HOOD_140', 'NEIGHBOURHOOD_140'])\n",
        "    crisis = crisis.rename(columns = {'EVENT_DATE': 'OCCUPANCY_DATE'})\n",
        "    crisis = crisis.groupby(['OCCUPANCY_DATE', 'EVENT_TYPE']).size().unstack(fill_value=0)\n",
        "    crisis.reset_index(inplace=True)\n",
        "    crisis = crisis.rename_axis(None, axis=1)\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE']).dt.date\n",
        "    crisis['OCCUPANCY_DATE'] = pd.to_datetime(crisis['OCCUPANCY_DATE'])\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] >= min_date]\n",
        "    crisis = crisis[crisis[\"OCCUPANCY_DATE\"] <= max_date]\n",
        "    crisis = pd.merge(date_df, crisis, on='OCCUPANCY_DATE', how='left')\n",
        "\n",
        "    #-------Final Data Prep-------#\n",
        "\n",
        "    #Merge the datasets together through date\n",
        "    big_data = pd.merge(big_data, big_weather, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, housing, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "    big_data = pd.merge(big_data, crisis, on = 'OCCUPANCY_DATE', how = 'inner')\n",
        "\n",
        "    big_data = big_data.sort_values(by='OCCUPANCY_DATE')\n",
        "\n",
        "    #Placing the bed and room occupancy column last\n",
        "    room_occupancy = big_data.pop('OCCUPANCY_RATE_ROOMS')\n",
        "    bed_occupancy = big_data.pop('OCCUPANCY_RATE_BEDS')\n",
        "    big_data['OCCUPANCY_RATE_BEDS'] = bed_occupancy\n",
        "    big_data['OCCUPANCY_RATE_ROOMS'] = room_occupancy\n",
        "\n",
        "    grouped_data = big_data.groupby('PROGRAM_ID')\n",
        "    shelter_data_frames = {}\n",
        "    for shelter_id, shelter_group in grouped_data:\n",
        "        shelter_data_frames[shelter_id] = shelter_group\n",
        "        shelter_data_frames[shelter_id]['OCCUPANCY_DATE'] = pd.to_datetime(shelter_data_frames[shelter_id]['OCCUPANCY_DATE'])\n",
        "\n",
        "    big_data.reset_index(inplace=True)\n",
        "    big_data = big_data.drop(columns = ['index'])\n",
        "\n",
        "    return big_data, shelter_data_frames"
      ],
      "metadata": {
        "id": "R7R3L8Ciq78s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the function to get the dataframe and hashmap"
      ],
      "metadata": {
        "id": "SmFYTI_mq_8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe, iso_data = loadData(links.copy(), links_weather.copy(), data_housing, data_crisis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0wGfbAtrBQ4",
        "outputId": "488f8cf3-949b-4ae7-ea49-745d4af2d126"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Daily shelter overnight occupancy.csv 11459\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2023.csv 48345\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2022.csv 49478\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/daily-shelter-overnight-service-occupancy-capacity-2021.csv 50944\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2024_P1D.csv 366\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2023_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2022_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/en_climate_daily_ON_6158355_2021_P1D.csv 365\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Housing.csv 62160\n",
            "Number of rows in the dataFrame: /content/drive/MyDrive/RBC | Borealis AI | Lets Solve IT/Datasets/Persons_in_Crisis_Calls_for_Service_Attended_Open_Data.csv 291991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Dataset to DataLoader\n",
        "\n",
        "With the proper dataframe, we will need to convert the dataset into a timeseries and then dataloader to feed into the model for training"
      ],
      "metadata": {
        "id": "VfYI4d4DtpwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function below is to grab the scaler to scale the dataset"
      ],
      "metadata": {
        "id": "S7W-X2jAua47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_scaler():\n",
        "    scaler = MinMaxScaler(feature_range = (-1, 1))\n",
        "    return scaler"
      ],
      "metadata": {
        "id": "_cZ1jYCZubtx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is a multivariate lstm, we will extract multiple features to train on. We will demonstrate models trained on two different set of features. The features used here will be OCCUPANCY_DATE, Max Temp (Â°C),OCCUPIED_PERCENTAGE."
      ],
      "metadata": {
        "id": "M7D_eJhTueP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_check(df):\n",
        "    df_ = dc(df)\n",
        "    for i in df_.columns:\n",
        "        if df_[i].isna().any():\n",
        "            avg = df_[i].mean()\n",
        "            df_.loc[:, i] = df_[i].fillna(avg)\n",
        "    return df_"
      ],
      "metadata": {
        "id": "KLuzefFruVSt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Convert the dataset into a dataloader, we will need to define a Dataset class for the time series"
      ],
      "metadata": {
        "id": "75S3LkVzujUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]"
      ],
      "metadata": {
        "id": "Vat87lPZukc2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function below will convert the dataframe to a time series for the model with proper hyperparameters as inputs"
      ],
      "metadata": {
        "id": "cgUNtpDnumvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_series_converter(df, scaler, n_past, n_future, train_test_split, batch_size):\n",
        "\n",
        "\n",
        "    df = dc(df)\n",
        "    df.set_index('OCCUPANCY_DATE', inplace=True)\n",
        "    df = df.astype(float)\n",
        "    scaler = scaler.fit(df)\n",
        "\n",
        "    df_scaled = scaler.transform(df)\n",
        "\n",
        "    num_feat = len([i for i in df])\n",
        "\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "\n",
        "    for i in range(n_past, len(df_scaled) - n_future + 1):\n",
        "        train_x.append(df_scaled[i - n_past:i, 0:df_scaled.shape[1]])\n",
        "        train_y.append(df_scaled[i: i + n_future, -1])\n",
        "\n",
        "    train_x, train_y = np.array(train_x), np.array(train_y)\n",
        "\n",
        "    split_index = int(len(train_x) * train_test_split)\n",
        "\n",
        "    X_train = train_x[:split_index]\n",
        "    X_test = train_x[split_index:]\n",
        "\n",
        "    Y_train = train_y[:split_index]\n",
        "    Y_test = train_y[split_index:]\n",
        "\n",
        "    X_train_ = X_train.reshape((-1, n_past, num_feat))\n",
        "    X_test_ = X_test.reshape((-1, n_past, num_feat))\n",
        "\n",
        "    X_train = torch.tensor(X_train).float()\n",
        "    Y_train = torch.tensor(Y_train).float()\n",
        "    X_test = torch.tensor(X_test).float()\n",
        "    Y_test = torch.tensor(Y_test).float()\n",
        "\n",
        "    train_Dataset = TimeSeriesDataset(X_train, Y_train)\n",
        "    test_Dataset = TimeSeriesDataset(X_test, Y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_Dataset, batch_size = batch_size, shuffle = True)\n",
        "    test_loader = DataLoader(test_Dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "kWTCC2ehupPP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create the timeseries for the model, we will have to define certain hyperparameters such as the number of steps we will be looking back at, the number of steps we will be looking into the future, our batch size, and the train test split ratio."
      ],
      "metadata": {
        "id": "kctfdshJurkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the Model\n",
        "\n",
        "Before training can begin, we will have to define the model architecture. Since the model is an lstm, it will contain LSTM architectures and also fully connected layers for the final output result. Please see the infrastructure below:"
      ],
      "metadata": {
        "id": "XnfWQOGQvB6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_stacked_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_stacked_layers = num_stacked_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_stacked_layers,\n",
        "                            batch_first=True)\n",
        "\n",
        "        self.fc = nn.Linear(hidden_size, output_size) #output_size being how many days in the future to predict\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_size)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ahjh8mXUvGT2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training Function. Call this function and pass in the model to begin training."
      ],
      "metadata": {
        "id": "I7aCZA5pvIqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def begin_training(model,num_epochs, train_loader, test_loader, loss, optimizer):\n",
        "\n",
        "    training_loss = []\n",
        "    validation_loss = []\n",
        "    average_validation_loss = []\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        #Training\n",
        "        model.train(True)\n",
        "        running_loss = 0\n",
        "        if epoch % 5 == 0:\n",
        "          print(\"Epoch: \" + str(epoch))\n",
        "\n",
        "        for batch_index, batch in enumerate(train_loader):\n",
        "            x_batch, y_batch = batch[0], batch[1]\n",
        "            output = model(x_batch)\n",
        "            loss_ = loss(output, y_batch)\n",
        "            running_loss += loss_.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss_.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "          print(\"Training Loss: \" + str(running_loss))\n",
        "        training_loss.append(running_loss)\n",
        "\n",
        "        #Validating\n",
        "        model.train(False)\n",
        "        vad_loss = 0\n",
        "\n",
        "        for batch_index, batch in enumerate(test_loader):\n",
        "            x_batch, y_batch = batch[0], batch[1]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output    = model(x_batch)\n",
        "                loss_     = loss(output, y_batch)\n",
        "                vad_loss += loss_.item()\n",
        "\n",
        "        validation_loss.append(vad_loss)\n",
        "        avg_loss_across_batches = vad_loss / len(test_loader)\n",
        "        average_validation_loss.append(avg_loss_across_batches)\n",
        "        if epoch % 5 == 0:\n",
        "          print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
        "          print('***************************************************')\n",
        "          print('\\n')\n",
        "\n",
        "\n",
        "    return model, training_loss, validation_loss, average_validation_loss\n"
      ],
      "metadata": {
        "id": "aypqC8DKvQH8"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}